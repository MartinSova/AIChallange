{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <i>Martin K. Sova</i></p>\n",
    "<h1><b>Introduction</b></h1>\n",
    "\n",
    "The solution to the Language Detection challenge is split into three parts: <u>Preprocessing training data</u>, <u>Building the models and predicting on test data</u>, and <u>Analysis of the models and conclusion</u>. My submission is split into the three sections in the order that they are listed in order to provide a clear structure of my approach and to accomplish an overall organized solution to the problem.\n",
    "\n",
    "<i>Note: I am aware that I did not use doc strings (although still provided inline-comments where it deemed appropriate). I made the decision to rather utilize markdown cells; since they are provided by jupyter notebook and are more pleasant to read, I detailed my reason for implementing certain modules and the explanations for my decision to apply given functions (including parameters and return values) in a markdown cell prior to their use.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Part 1</b></h1>\n",
    "<h2><u>Preprocessing training data</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 1.1: Imports</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>re</code> module is imported to make use of regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>csv</code> module is utilized for the import format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>pandas</code> module provides an array of data analysis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>os</code> module allows for simple functions such opening files; 'a portable way of using operating system dependent functionality' (https://docs.python.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make use of the <code>glob module</code>, as seen in the <code>loadData</code> function for instance. The advantage of <code>glob</code> is that I can find all the pathnames that match a given pattern (absolute or relative). We observe the implementation of <code>glob.glob(path)</code> to find a - potentially empty - list of path names that match the parameter <code>path_name</code>; for example, <code>glob.glob('thesis/*.doc')</code> will return a list of all pathnames for .doc files found in the folder <i>thesis</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>random</code> module is imported to provide a pseudo-random number generator (used in Section 1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 1.2: Reading data from files </b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>loadData(path_name)</code> function provides a method for extracting relevant text from the file corresponding to the pathname given by the parameter <code>path_name</code>.\n",
    "\n",
    "The parameter <code>path_name</code> determines the path to files from which data should be extracted.\n",
    "    \n",
    "The function returns a list of extracted words from files that have a matching path to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path_name):\n",
    "\n",
    "    # Create a list for all words found in the specified text file.\n",
    "    text_return = []\n",
    "    \n",
    "    # All path names that match the parameter.\n",
    "    all_files = glob.glob(path_name)\n",
    "    # The use of a smaller sample is justified in the conclusion.\n",
    "    num_delete = int(len(all_files) / 1.25)\n",
    "    num_keep = len(all_files) - num_delete\n",
    "    files = set(random.sample(all_files, num_keep))\n",
    "    files = [i for i in all_files if i in files]\n",
    "    \n",
    "    # Iterate over all found files.\n",
    "    for f in files:\n",
    "        input_data = open(f, encoding='utf8')\n",
    "        input_data = input_data.read()\n",
    "        # Obtain all words from the text file for pathname 'f'.\n",
    "        extracted_text = input_data.split(' ')\n",
    "        for word in extracted_text:\n",
    "            # Remove any unwanted words before returning the final extracted text.\n",
    "            if word !='ID' and word !='SPEAKER' and word !='LANGUAGE' and word !='NAME' and word !='CHAPTER' and  word !='<P>':\n",
    "                # If word satisfies the conditions append to the list of extracted words to be returned.\n",
    "                text_return.append(word)\n",
    "    # Return the final list of extracted words.\n",
    "    return(text_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 1.3: Denoising the training (and testing) data</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>denoise(input_data, data_type)</code> function provides a procedure for removing any unwanted noise from data.\n",
    "\n",
    "The parameter <code>input_data</code> is the source data to be denoised, and the parameter <code>data_type</code> determines whether the source data is the training or testing data (which, as observed, determines the variable <code>i</code>).\n",
    "\n",
    "The function returns the text data after removing all noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<i>The <code>pandas.DataFrame.drop_duplicates()</code> function is utilized to remove any duplicate rows from the DataFrame (the use of DataFrame is defined in Step 1.6).</i>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<i>The <code>pandas.DataFrame.dropna()</code> is utilized to drop NA entries from the DataFrame; in other words, enables to remove any missing values.</i>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(input_data, data_type):\n",
    "    if data_type == 'train':\n",
    "        i = 0\n",
    "    else:\n",
    "        i = 1\n",
    "    input_data[i] = input_data[i].str.replace('[=<>\":-;.,\\(\\)]', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('[0-9]', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('\\s', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('<P>', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('/', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('ID', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('NAME', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('SPEAKER', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('CHAPTER', ' ')\n",
    "    input_data[i] = input_data[i].str.replace('LANGUAGE', ' ')\n",
    "    input_data[i] = input_data[i].str.strip()\n",
    "    input_data = input_data[input_data[i] != '']\n",
    "    input_data = input_data.drop_duplicates()\n",
    "    input_data =input_data.dropna()\n",
    "    return(input_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising the extracted text encompasses the removal of characters such as digits and punctuations from the training data. We emphasize that it is important to perform the equivalent denoising procedure for both the training and test data; that is, to also get rid of digits and punctuations, which also add noise to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 1.4: Define the dataset labels</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a list of labels as appear in the European Parliament Proceedings Parallel Corpus text dataset (e.g. folder names) enables for more intuitive implementation; listing the labels to be classified proves to be hugely advantageous, such as allowing iteration as observed in Steps 1.6 (also utilized in Step 2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are labels to be classified.\n",
    "all_labels = ['bg','cs','da','de','el','en','es','et','fi','fr','hu','it','lt','lv','nl','pl','pt','ro','sk','sl','sv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, there are 21 languages for which detection must be supported, so it is imperative that the classifier accurately recognizes all 21 categorical variables.\n",
    "\n",
    "List of all languages (in order as they appear in the list variable <code>all_labels</code>): <i> Bulgarian, Czech, Danish, German, Greek, English, Spanish, Estonian, Finnish, French, Hungarian, Italian, Lithuanian, Latvian, Dutch, Polish, Portuguese, Romanian, Slovak, Slovenian, Swedish. </bi>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 1.5: Function to return a string of all words of a language</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>convert(test_series)</code> function provides a method to convert all words of a language in a Series to a sequence, and return the word concatenation of the strings in the sequence joined by a given <code>str</code> seperator.\n",
    "\n",
    "The parameter <code>text</code> is the Series containing all words of a language.\n",
    "\n",
    "The return value <code>to_string</code> of the function is the string containing all words joined by the <code>str</code> seperator ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text): \n",
    "    text_list = text[0].tolist()\n",
    "    to_string = ' '.join(text_list)\n",
    "    return(to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <code>pandas.Series.tolist()</code> is used to perform a series to list conversion.\n",
    "\n",
    "Here the <code>join()</code> function is utilized to return a string by joining the string elements of a sequence with a <code>str</code> separator; in this case, a space ' '."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 1.6: <i>Realizing Steps 1.2 and 1.3</i></b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing functions <code>loadData(path_name)</code> and <code>denoise(input_data, data_type)</code> to get training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>DataFrame()</code> function of the <code>pandas</code> module serves to enable representation of the extracted data as a size-mutable tabular data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have placed all data labels in a list, which means that we can call the <code>loadData</code> and <code>denoise</code> functions from within a for loop to extract and denoise all data and store it in a list with just a few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results = {}\n",
    "for label in all_labels:\n",
    "    path_name = 'txt/' + label + '/*.txt'\n",
    "    data = loadData(path_name)\n",
    "    data_df = pandas.DataFrame(data)\n",
    "    data_denoised = denoise(data_df, 'train')\n",
    "    data_results[label] = data_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 1.7:<i> Realizing Step 1.5</i></b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for d in data_results:\n",
    "    results.append(convert(data_results[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 1.8: A single dataframe</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert the list containing strings of all languages defined in Step 1.7 into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pandas.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Одобряване на протокола от предишното заседани...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schválení zápisu z předchozího zasedání viz zá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NL Formanden    Jeg giver ordet til fru Maes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Altfahrzeuge Der Präsident   Nach der Tagesord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>΄Εvαρξη της ετήσιας συvόδoυ Πρόεδρος Κηρύσσω τ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Resumption of the session President   I declar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aprobación del Acta de la sesión anterior El P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Uurimiskomisjoni ja ajutise komisjoni moodusta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Romuajoneuvot Puhemies   Esityslistalla on seu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Adoption du procès-verbal de la séance précéde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A bizottságok és a küldöttségek tagjai lásd je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ripresa della sessione Presidente   Dichiaro r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Peticijos žr protokolą Bendru sprendimu priimt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Komiteju un delegāciju sastāvs sk protokolu Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hervatting van de zitting De Voorzitter   Ik v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Zatwierdzenie protokołu z poprzedniego posiedz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Homenagem póstuma NL Presidente   Faleceu onte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Petiţii a se vedea procesul-verbal Ordinea de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Program rokovania na nasledujúci deň pozri záp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pisne izjave člen glej zapisnik Dnevni red nas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Justering av protokollet från föregående samma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   Одобряване на протокола от предишното заседани...\n",
       "1   Schválení zápisu z předchozího zasedání viz zá...\n",
       "2   NL Formanden    Jeg giver ordet til fru Maes s...\n",
       "3   Altfahrzeuge Der Präsident   Nach der Tagesord...\n",
       "4   ΄Εvαρξη της ετήσιας συvόδoυ Πρόεδρος Κηρύσσω τ...\n",
       "5   Resumption of the session President   I declar...\n",
       "6   Aprobación del Acta de la sesión anterior El P...\n",
       "7   Uurimiskomisjoni ja ajutise komisjoni moodusta...\n",
       "8   Romuajoneuvot Puhemies   Esityslistalla on seu...\n",
       "9   Adoption du procès-verbal de la séance précéde...\n",
       "10  A bizottságok és a küldöttségek tagjai lásd je...\n",
       "11  Ripresa della sessione Presidente   Dichiaro r...\n",
       "12  Peticijos žr protokolą Bendru sprendimu priimt...\n",
       "13  Komiteju un delegāciju sastāvs sk protokolu Do...\n",
       "14  Hervatting van de zitting De Voorzitter   Ik v...\n",
       "15  Zatwierdzenie protokołu z poprzedniego posiedz...\n",
       "16  Homenagem póstuma NL Presidente   Faleceu onte...\n",
       "17  Petiţii a se vedea procesul-verbal Ordinea de ...\n",
       "18  Program rokovania na nasledujúci deň pozri záp...\n",
       "19  Pisne izjave člen glej zapisnik Dnevni red nas...\n",
       "20  Justering av protokollet från föregående samma..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Part 2</b></h1>\n",
    "<h2><u>Building the models and predicting on the test data</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 2.1: Imports</b></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>sklearn.pipeline</code> module is imported to utilize the <code>Pipeline</code> of transforms with a final estimator.\n",
    "\n",
    "The reason why I make use of the pipeline is to execute steps that can be cross-validated together while passing varying parameters; it allows to set parameters of the different steps using names and parameter names.\n",
    "\n",
    "scikit-learn.org states \"Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods.\" We observe that in the final estimator we only need to implement <code>.fit()</code>. \n",
    "\n",
    "As observed in Step 2.2, we set a list parameter of (name, transform) tuples (implementing fit/transform) chained in a specific order, where the last object is the estimator.\n",
    "\n",
    "The <code>memory</code> parameter of the <code>sklearn.pipeline.Pipeline(steps, memory=None)</code> function enables the transformers in the pipeline to be cached using the memory argument; however, this is not utilized in my implementation of the <code>pipeline</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>CountVectorizer</code> function of the <code>sklearn.feature_extraction.text</code> class is used for the conversion of \"a collection of text documents to a matrix of token counts\" (http://scikit-learn.org/), which performs well on nested objects or simple estimators (such as pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>TfidfVectorizer</code> function of the <code>sklearn.feature_extraction.text</code> class is used for the conversion of \"a collection of raw documents to a matrix of TF-IDF features\" (http://scikit-learn.org/), which also performs well on nested objects or simple estimators (such as pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>TfidfTransformer</code> function of the <code>sklearn.feature_extraction.text</code> class is used for the conversion of \"a count matrix to a normalized tf or tf-idf representation\" (http://scikit-learn.org/), which also performs well on nested objects or simple estimators (such as pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>LogisticRegression</code> classifier of the <code>sklearn.linear_model</code> class is imported because the analysis for the frequency of words and characters in a given language will be achieved with a logistic regressing model.\n",
    "\n",
    "The n-gram models tested for, as observed in Step 2.2, include the analysis of: <i>1-gram word frequency, 2-gram word frequency, 1-gram character frequency, 2-gram character frequency, and 4-gram character frequency</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 2.2: Training the data</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the model and train it with the extracted training data.\n",
    "\n",
    "The pipeline is used to implement the 5 models detailed in Step 2.1.\n",
    "\n",
    "In order to achieve generalization to previously unseen data, I have set the inverse of regularization strength to 1.0 for each model. \n",
    "\n",
    "Further, <b>L2 regularization</b> is utilized to avoid overfitting of the models to training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "info_pipeline = [('model_1_char', 'char', (1, 1)), ('model_1_word', 'word', (1, 1)),\n",
    "                 ('model_2_char', 'char', (1, 2)), ('model_4_char', 'char', (1, 4)), ('model_2_word', 'word', (1, 2))]\n",
    "for values in info_pipeline: \n",
    "    k = Pipeline([('vect', CountVectorizer(ngram_range = (values[2][0], values[2][1]), analyzer = values[1])),\n",
    "                  ('tfidf', TfidfTransformer(use_idf = False)), ('lrg', LogisticRegression(n_jobs = -1))])\n",
    "    models[values[0]] = k.fit(training_data[0], all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, models are built and stored in a dictionary from within a for loop, as steps taken for each model are similar with just few varying parameters, which are retrieved from the <code>info_pipeline</code> list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Step 2.3: Preparing test data</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing function <code>denoise(input_date, data_type)</code> defined in Step 1.3, we prepare the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pandas.read_csv('europarl.test', sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the function <code>read_csv</code> is used to read the test file into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[1]= test_data[1].str.replace('\\(.*?\\)','')\n",
    "test_data= denoise(test_data, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 2.4: Predictions on test data</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to intuitively perform predictions on the test data from within a for loop as the trained models are stored in a dictionary; the 'model' substring of the key is replaced with 'prediction' using the <code>replace</code> function and the predictions are therafter stored in a new dictionary for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the <code>predict</code> function to predict the target values for individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for model in models:\n",
    "    label = model.replace('model','prediction')\n",
    "    predictions[label] = models[model].predict(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Part 3</b></h1>\n",
    "<h2><u>Analysis of the models and conclusion</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 3.1: Imports</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>accuracy_score</code> function of the <code>sklearn.metrics</code> class is imported to compute accuracy classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>confusion_matrix</code> function of the <code>sklearn.metrics</code> class is imported to compute the confusion matrix to evaluate the accuracy of a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>classification_report</code> function of the <code>sklearn.metrics</code> class allows for a text report that demonstrates the fundamental classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>precision_recall_fscore_support</code> function of the <code>sklearn.metrics</code> class is imported to compute the recall, precision, support and F-measure for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>f1_score</code> function of the <code>sklearn.metrics</code> class is imported to compute the F1 score (the balanced F-measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Step 3.2: Analysis</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification report and accuracy score for a 4-gram character model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00       997\n",
      "         cs       0.80      0.95      0.87       993\n",
      "         da       0.88      0.95      0.91       994\n",
      "         de       0.69      0.99      0.81       993\n",
      "         el       1.00      1.00      1.00       988\n",
      "         en       1.00      0.70      0.82       998\n",
      "         es       0.99      0.76      0.86       996\n",
      "         et       0.87      0.89      0.88       993\n",
      "         fi       0.84      1.00      0.91       995\n",
      "         fr       0.99      0.88      0.93       999\n",
      "         hu       0.90      0.99      0.94       998\n",
      "         it       0.98      0.81      0.88       996\n",
      "         lt       0.93      0.97      0.95       995\n",
      "         lv       1.00      0.97      0.98       978\n",
      "         nl       0.88      0.89      0.89       999\n",
      "         pl       0.97      0.99      0.98       997\n",
      "         pt       0.86      0.95      0.90       996\n",
      "         ro       0.85      0.97      0.91       927\n",
      "         sk       0.98      0.68      0.81       929\n",
      "         sl       0.98      0.93      0.95       998\n",
      "         sv       0.98      0.85      0.91       996\n",
      "\n",
      "avg / total       0.92      0.91      0.91     20755\n",
      "\n",
      "0.91033485907\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_data[0], predictions['prediction_4_char']))\n",
    "print (accuracy_score(test_data[0], predictions['prediction_4_char']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'><u>Results:</u></font>\n",
    "\n",
    "The calculated prediction accuracy accomplished on the test data with a model trained with the 4-gram character logistic regression model is: <b> 91.033%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification report and accuracy score for a 2-gram character model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00       997\n",
      "         cs       0.74      0.94      0.83       993\n",
      "         da       0.86      0.93      0.89       994\n",
      "         de       0.66      0.98      0.79       993\n",
      "         el       1.00      1.00      1.00       988\n",
      "         en       0.99      0.65      0.78       998\n",
      "         es       0.97      0.67      0.80       996\n",
      "         et       0.86      0.85      0.86       993\n",
      "         fi       0.81      0.99      0.89       995\n",
      "         fr       0.98      0.83      0.90       999\n",
      "         hu       0.90      0.99      0.94       998\n",
      "         it       0.94      0.75      0.83       996\n",
      "         lt       0.91      0.95      0.93       995\n",
      "         lv       0.99      0.97      0.98       978\n",
      "         nl       0.79      0.86      0.83       999\n",
      "         pl       0.97      0.99      0.98       997\n",
      "         pt       0.80      0.93      0.86       996\n",
      "         ro       0.82      0.95      0.88       927\n",
      "         sk       0.97      0.57      0.72       929\n",
      "         sl       0.97      0.91      0.94       998\n",
      "         sv       0.97      0.82      0.89       996\n",
      "\n",
      "avg / total       0.90      0.88      0.88     20755\n",
      "\n",
      "0.883401589978\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_data[0], predictions['prediction_2_char']))\n",
    "print (accuracy_score(test_data[0], predictions['prediction_2_char']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'><u>Results:</u></font>\n",
    "\n",
    "The calculated prediction accuracy accomplished on the test data with a model trained with the 2-gram character logistic regression model is: <b> 88.340%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification report and accuracy score for a 1-gram character model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00       997\n",
      "         cs       0.61      0.93      0.74       993\n",
      "         da       0.82      0.83      0.82       994\n",
      "         de       0.55      0.95      0.70       993\n",
      "         el       1.00      1.00      1.00       988\n",
      "         en       0.94      0.47      0.63       998\n",
      "         es       0.79      0.48      0.60       996\n",
      "         et       0.80      0.75      0.77       993\n",
      "         fi       0.75      0.98      0.85       995\n",
      "         fr       0.92      0.55      0.69       999\n",
      "         hu       0.87      0.98      0.92       998\n",
      "         it       0.84      0.52      0.64       996\n",
      "         lt       0.78      0.93      0.85       995\n",
      "         lv       0.98      0.90      0.94       978\n",
      "         nl       0.58      0.87      0.69       999\n",
      "         pl       0.96      0.97      0.96       997\n",
      "         pt       0.66      0.85      0.74       996\n",
      "         ro       0.74      0.83      0.78       927\n",
      "         sk       0.87      0.33      0.47       929\n",
      "         sl       0.94      0.79      0.86       998\n",
      "         sv       0.93      0.73      0.82       996\n",
      "\n",
      "avg / total       0.83      0.79      0.79     20755\n",
      "\n",
      "0.792628282342\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_data[0], predictions['prediction_1_char']))\n",
    "print (accuracy_score(test_data[0], predictions['prediction_1_char']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'><u>Results:</u></font>\n",
    "\n",
    "The calculated prediction accuracy accomplished on the test data with a model trained with the 1-gram character logistic regression model is: <b> 79.263%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification report and accuracy score for a 2-gram word model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00       997\n",
      "         cs       0.84      0.50      0.63       993\n",
      "         da       0.97      0.89      0.93       994\n",
      "         de       0.93      0.97      0.95       993\n",
      "         el       0.63      1.00      0.78       988\n",
      "         en       0.87      0.97      0.92       998\n",
      "         es       0.85      0.74      0.79       996\n",
      "         et       0.95      0.87      0.91       993\n",
      "         fi       0.90      0.83      0.86       995\n",
      "         fr       0.73      0.85      0.78       999\n",
      "         hu       0.99      0.96      0.97       998\n",
      "         it       0.89      0.94      0.92       996\n",
      "         lt       0.98      0.84      0.90       995\n",
      "         lv       0.80      0.87      0.83       978\n",
      "         nl       0.56      0.96      0.71       999\n",
      "         pl       0.95      0.80      0.87       997\n",
      "         pt       0.54      0.69      0.60       996\n",
      "         ro       1.00      0.81      0.89       927\n",
      "         sk       0.92      0.64      0.75       929\n",
      "         sl       0.78      0.47      0.58       998\n",
      "         sv       0.98      0.92      0.95       996\n",
      "\n",
      "avg / total       0.86      0.83      0.83     20755\n",
      "\n",
      "0.834160443267\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_data[0], predictions['prediction_2_word']))\n",
    "print (accuracy_score(test_data[0], predictions['prediction_2_word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'><u>Results:</u></font>\n",
    "\n",
    "The calculated prediction accuracy accomplished on the test data with a model trained with the 2-gram word logistic regression model is: <b> 83.416%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification report and accuracy score for a 1-gram word model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00       997\n",
      "         cs       0.84      0.51      0.63       993\n",
      "         da       0.97      0.89      0.93       994\n",
      "         de       0.93      0.97      0.95       993\n",
      "         el       0.64      1.00      0.78       988\n",
      "         en       0.88      0.97      0.92       998\n",
      "         es       0.85      0.74      0.79       996\n",
      "         et       0.95      0.87      0.91       993\n",
      "         fi       0.90      0.83      0.86       995\n",
      "         fr       0.73      0.85      0.79       999\n",
      "         hu       0.99      0.96      0.97       998\n",
      "         it       0.90      0.94      0.92       996\n",
      "         lt       0.97      0.84      0.90       995\n",
      "         lv       0.80      0.87      0.83       978\n",
      "         nl       0.56      0.95      0.71       999\n",
      "         pl       0.95      0.80      0.87       997\n",
      "         pt       0.54      0.69      0.61       996\n",
      "         ro       1.00      0.81      0.90       927\n",
      "         sk       0.92      0.65      0.76       929\n",
      "         sl       0.78      0.47      0.59       998\n",
      "         sv       0.98      0.92      0.95       996\n",
      "\n",
      "avg / total       0.86      0.84      0.84     20755\n",
      "\n",
      "0.835991327391\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_data[0], predictions['prediction_1_word']))\n",
    "print (accuracy_score(test_data[0], predictions['prediction_1_word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'><u>Results:</u></font>\n",
    "\n",
    "The calculated prediction accuracy accomplished on the test data with a model trained with the 1-gram word logistic regression model is: <b> 83.599%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3.3: Further analysis with a crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further analyse the results using a crosstab, which gives an idea about the correlation between the languages (false negatives and false positives are displayed).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the model trained with the 4-gram word logistic regression model, which returned the highest prediction accuracy of 90.870%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pandas.crosstab(test_data[0], predictions['prediction_4_char'], rownames=[\"V\"], colnames=[\"PV\"], margins=True)\n",
    "crosstab = pandas.DataFrame(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that, for example, in Italian 20 words were missclassified as Portuguese.\n",
    "\n",
    "Most strings were missclassified in Slovakian as Czech: 212 in total. I grew up in Prague, Czech Republic, for 19 years and can vouch for the similarity between the two languages, being able to communicate with my Slovak friends almost seamlessly while speaking our respective languages. This is due to the shared history of the two countries.\n",
    "\n",
    "The trends between other languages can be observed below. 'V' signifies the actual values and 'PV' signifies the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>PV</th>\n",
       "      <th>bg</th>\n",
       "      <th>cs</th>\n",
       "      <th>da</th>\n",
       "      <th>de</th>\n",
       "      <th>el</th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "      <th>et</th>\n",
       "      <th>fi</th>\n",
       "      <th>fr</th>\n",
       "      <th>...</th>\n",
       "      <th>lt</th>\n",
       "      <th>lv</th>\n",
       "      <th>nl</th>\n",
       "      <th>pl</th>\n",
       "      <th>pt</th>\n",
       "      <th>ro</th>\n",
       "      <th>sk</th>\n",
       "      <th>sl</th>\n",
       "      <th>sv</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bg</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0</td>\n",
       "      <td>943</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>940</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>696</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>758</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>884</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>880</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>963</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lv</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>952</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>943</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ro</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk</th>\n",
       "      <td>0</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>633</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sl</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>844</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>997</td>\n",
       "      <td>1181</td>\n",
       "      <td>1064</td>\n",
       "      <td>1418</td>\n",
       "      <td>989</td>\n",
       "      <td>699</td>\n",
       "      <td>766</td>\n",
       "      <td>1012</td>\n",
       "      <td>1183</td>\n",
       "      <td>885</td>\n",
       "      <td>...</td>\n",
       "      <td>1037</td>\n",
       "      <td>956</td>\n",
       "      <td>1012</td>\n",
       "      <td>1017</td>\n",
       "      <td>1099</td>\n",
       "      <td>1053</td>\n",
       "      <td>643</td>\n",
       "      <td>950</td>\n",
       "      <td>859</td>\n",
       "      <td>20755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "PV    bg    cs    da    de   el   en   es    et    fi   fr  ...      lt   lv  \\\n",
       "V                                                           ...                \n",
       "bg   997     0     0     0    0    0    0     0     0    0  ...       0    0   \n",
       "cs     0   943     0     2    0    0    0     4     6    0  ...       5    0   \n",
       "da     0     0   940    17    1    0    0     4     2    0  ...       0    1   \n",
       "de     0     0     4   979    0    0    0     4     3    0  ...       0    0   \n",
       "el     0     0     0     0  988    0    0     0     0    0  ...       0    0   \n",
       "en     0     7    16   156    0  696    2    23    11    4  ...       3    1   \n",
       "es     0     2     3    43    0    0  758    14     5    1  ...       3    0   \n",
       "et     0     0     2    11    0    0    0   884    75    0  ...       9    0   \n",
       "fi     0     0     0     2    0    0    0     2   991    0  ...       0    0   \n",
       "fr     0     2    12    53    0    1    2     8     8  880  ...       3    0   \n",
       "hu     0     0     0     1    0    0    0     0     1    0  ...       2    0   \n",
       "it     0     1     9    16    0    0    1    13     9    0  ...       9    0   \n",
       "lt     0     2     0     1    0    0    0    11     7    0  ...     963    1   \n",
       "lv     0     0     0     2    0    0    0     3     4    0  ...      13  952   \n",
       "nl     0     3     3    85    0    0    0     5     3    0  ...       0    0   \n",
       "pl     0     0     0     3    0    0    0     1     3    0  ...       0    0   \n",
       "pt     0     1     6    12    0    0    2     9     2    0  ...       1    0   \n",
       "ro     0     0     1     9    0    0    0     6     5    0  ...       3    0   \n",
       "sk     0   212     2     4    0    0    0     6    11    0  ...       9    1   \n",
       "sl     0     8     8     1    0    0    0    11     7    0  ...      13    0   \n",
       "sv     0     0    58    21    0    2    1     4    30    0  ...       1    0   \n",
       "All  997  1181  1064  1418  989  699  766  1012  1183  885  ...    1037  956   \n",
       "\n",
       "PV     nl    pl    pt    ro   sk   sl   sv    All  \n",
       "V                                                  \n",
       "bg      0     0     0     0    0    0    0    997  \n",
       "cs      0     4     1     1    3    0    0    993  \n",
       "da      9     0     1     0    0    0    9    994  \n",
       "de      1     0     0     0    0    0    0    993  \n",
       "el      0     0     0     0    0    0    0    988  \n",
       "en     35     5    11     9    0    0    5    998  \n",
       "es     22     0   117    18    0    0    0    996  \n",
       "et      7     0     0     0    1    3    1    993  \n",
       "fi      0     0     0     0    0    0    0    995  \n",
       "fr     10     1     0    14    1    0    0    999  \n",
       "hu      0     0     0     0    0    1    0    998  \n",
       "it      7     0    20   103    1    0    0    996  \n",
       "lt      1     1     2     0    0    4    0    995  \n",
       "lv      0     2     2     0    0    0    0    978  \n",
       "nl    893     1     0     1    0    1    0    999  \n",
       "pl      1   988     0     0    0    0    0    997  \n",
       "pt      7     0   943     9    0    0    0    996  \n",
       "ro      2     0     2   896    0    2    0    927  \n",
       "sk      3    13     0     0  633   12    0    929  \n",
       "sl     12     1     0     1    3  927    0    998  \n",
       "sv      2     1     0     1    1    0  844    996  \n",
       "All  1012  1017  1099  1053  643  950  859  20755  \n",
       "\n",
       "[22 rows x 22 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3.4: Conclusion </h3>\n",
    "\n",
    "A fifth of text files were randomly chosen for each language for the analysis section due to the size of the European Parliment Corpus (5 GB), which proved to be less time expensive when performing tests (in cases other than for the purpose of this challenge, this could be easily adjusted to account for the entire dataset in the <code>loadData</code> function); the use of the entire dataset suggests the necessity of Big Data analysis. Despite using a subset of the data we are still able to generalize to unseen data and achieve a prediction accuracy of 91.033%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
